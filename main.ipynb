{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -U matplotlib\n",
    "# %pip install SimpleITK\n",
    "# %pip install ipywidgets\n",
    "# %pip install opencv-python\n",
    "# %pip install antspyx\n",
    "# %pip install tf-keras\n",
    "# %pip install --upgrade tensorflow-probability\n",
    "# %pip install antspynet\n",
    "# %pip install templateflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PATNO</th>\n",
       "      <th>COHORT</th>\n",
       "      <th>COHORT_DEFINITION</th>\n",
       "      <th>ENROLL_DATE</th>\n",
       "      <th>ENROLL_STATUS</th>\n",
       "      <th>STATUS_DATE</th>\n",
       "      <th>ENROLL_AGE</th>\n",
       "      <th>INEXPAGE</th>\n",
       "      <th>AV133STDY</th>\n",
       "      <th>TAUSTDY</th>\n",
       "      <th>...</th>\n",
       "      <th>COMMENTS</th>\n",
       "      <th>CONDATE</th>\n",
       "      <th>ENRLPINK1</th>\n",
       "      <th>ENRLPRKN</th>\n",
       "      <th>ENRLSRDC</th>\n",
       "      <th>ENRLHPSM</th>\n",
       "      <th>ENRLRBD</th>\n",
       "      <th>ENRLLRRK2</th>\n",
       "      <th>ENRLSNCA</th>\n",
       "      <th>ENRLGBA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3000</td>\n",
       "      <td>2</td>\n",
       "      <td>Healthy Control</td>\n",
       "      <td>02/2011</td>\n",
       "      <td>Enrolled</td>\n",
       "      <td>05/2021</td>\n",
       "      <td>69.1</td>\n",
       "      <td>INEXHC</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10/2023</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3001</td>\n",
       "      <td>1</td>\n",
       "      <td>Parkinson's Disease</td>\n",
       "      <td>03/2011</td>\n",
       "      <td>Enrolled</td>\n",
       "      <td>09/2021</td>\n",
       "      <td>65.1</td>\n",
       "      <td>INEXPD</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10/2023</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3002</td>\n",
       "      <td>1</td>\n",
       "      <td>Parkinson's Disease</td>\n",
       "      <td>03/2011</td>\n",
       "      <td>Enrolled</td>\n",
       "      <td>09/2021</td>\n",
       "      <td>67.6</td>\n",
       "      <td>INEXPD</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10/2023</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3003</td>\n",
       "      <td>1</td>\n",
       "      <td>Parkinson's Disease</td>\n",
       "      <td>04/2011</td>\n",
       "      <td>Enrolled</td>\n",
       "      <td>01/2022</td>\n",
       "      <td>56.7</td>\n",
       "      <td>INEXPD</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10/2023</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3004</td>\n",
       "      <td>2</td>\n",
       "      <td>Healthy Control</td>\n",
       "      <td>04/2011</td>\n",
       "      <td>Enrolled</td>\n",
       "      <td>01/2022</td>\n",
       "      <td>59.4</td>\n",
       "      <td>INEXHC</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10/2023</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   PATNO  COHORT    COHORT_DEFINITION ENROLL_DATE ENROLL_STATUS STATUS_DATE  \\\n",
       "0   3000       2      Healthy Control     02/2011      Enrolled     05/2021   \n",
       "1   3001       1  Parkinson's Disease     03/2011      Enrolled     09/2021   \n",
       "2   3002       1  Parkinson's Disease     03/2011      Enrolled     09/2021   \n",
       "3   3003       1  Parkinson's Disease     04/2011      Enrolled     01/2022   \n",
       "4   3004       2      Healthy Control     04/2011      Enrolled     01/2022   \n",
       "\n",
       "   ENROLL_AGE INEXPAGE  AV133STDY  TAUSTDY  ...  COMMENTS  CONDATE  ENRLPINK1  \\\n",
       "0        69.1   INEXHC        0.0      0.0  ...       NaN  10/2023        0.0   \n",
       "1        65.1   INEXPD        0.0      0.0  ...       NaN  10/2023        0.0   \n",
       "2        67.6   INEXPD        0.0      0.0  ...       NaN  10/2023        0.0   \n",
       "3        56.7   INEXPD        0.0      0.0  ...       NaN  10/2023        0.0   \n",
       "4        59.4   INEXHC        0.0      0.0  ...       NaN  10/2023        0.0   \n",
       "\n",
       "  ENRLPRKN  ENRLSRDC ENRLHPSM  ENRLRBD  ENRLLRRK2  ENRLSNCA  ENRLGBA  \n",
       "0      0.0       0.0        0        0          0         0        0  \n",
       "1      0.0       1.0        0        0          0         0        0  \n",
       "2      0.0       1.0        0        0          0         0        0  \n",
       "3      0.0       1.0        0        0          0         0        0  \n",
       "4      0.0       0.0        0        0          0         0        0  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://ida.loni.usc.edu/pages/access/studyData.jsp?categoryId=2&subCategoryId=55\n",
    "df = pd.read_csv('Participant_Status_22Mar2024.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df[condition]\n",
    "df[\"COHORT\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"ENROLL_STATUS\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = df[\"ENROLL_STATUS\"].isin([\"Complete\", \"Withdrew\", \"Enrolled\", \"Baseline\"])\n",
    "df[condition][\"COHORT\"].value_counts()\n",
    "#her"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = df[\"ENROLL_STATUS\"].isin([\"Enrolled\"])\n",
    "df[condition][\"COHORT\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = df[\"ENROLL_STATUS\"].isin([\"Complete\", \"Withdrew\"])\n",
    "df[condition][\"COHORT\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = df[\"ENROLL_STATUS\"].isin([\"Complete\", \"Withdrew\", \"Baseline\"])\n",
    "df[condition][\"COHORT\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.asarray(Image.open('status.png'))\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"ENROLL_STATUS\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = df[\"ENROLL_STATUS\"].isin([\"Complete\", \"Withdrew\"])\n",
    "sample_1 = df[(df[\"COHORT\"] == 1) & condition]\n",
    "sample_2 = df[(df[\"COHORT\"] == 2) & condition]\n",
    "sample_3 = df[(df[\"COHORT\"] == 3) & condition]\n",
    "sample_4 = df[(df[\"COHORT\"] == 4) & condition]\n",
    "\n",
    "df2 = pd.concat([sample_1, sample_2, sample_3, sample_4])\n",
    "df2[\"COHORT\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.asarray(Image.open('cohort.png'))\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#89 at least 2 visits\n",
    "#204 any date\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "coherts = {}\n",
    "df4 = df[df[\"ENROLL_STATUS\"].isin([\"Complete\", \"Withdrew\"])]\n",
    "total = 0\n",
    "total_to_remove = 0\n",
    "for p in os.listdir('PPMI'):\n",
    "    try:\n",
    "        patno = int(p)\n",
    "    except ValueError:\n",
    "        print(\"error\", p) \n",
    "        continue\n",
    "\n",
    "    matching_rows = df4[df4[\"PATNO\"] == patno]\n",
    "    if not matching_rows.empty:\n",
    "        total += 1\n",
    "        coherts[patno] = matching_rows[\"COHORT\"].values[0]\n",
    "    else:\n",
    "        path_to_remove = os.path.join(\"PPMI\", str(patno))\n",
    "        shutil.rmtree(path_to_remove)\n",
    "        # print(\"removed\", patno)\n",
    "        total_to_remove +=1\n",
    "\n",
    "# for c in range(1, 5):\n",
    "#     print(f\"COHORT {c}: {len([k for k, v in coherts.items() if v == c])}\")\n",
    "\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_to_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export TEMPLATEFLOW_HOME=\"C:/Users/andre/parkinson/templates\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import SimpleITK as sitk\n",
    "from antspynet.utilities import brain_extraction\n",
    "from templateflow import api as tflow\n",
    "import ants\n",
    "\n",
    "template = tflow.get('MNI152Lin', desc=None, resolution=1, suffix='T2w', extension='nii.gz')\n",
    "mni_template_path = str(template)\n",
    "mni_nifti = ants.image_read(mni_template_path)\n",
    "\n",
    "template_brain = brain_extraction(mni_nifti, modality=\"t2\", verbose=False)\n",
    "\n",
    "template_brain_mask = ants.get_mask(template_brain, low_thresh=0.5)\n",
    "template_extracted_brain = ants.mask_image(mni_nifti, template_brain_mask)\n",
    "\n",
    "\n",
    "def convert_dicom_to_nifti_and_extract_brain(dicom_directory, output_directory):\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "    \n",
    "    # Read DICOM files\n",
    "    reader = sitk.ImageSeriesReader()\n",
    "    dicom_names = reader.GetGDCMSeriesFileNames(dicom_directory)\n",
    "    reader.SetFileNames(dicom_names)\n",
    "    image = reader.Execute()\n",
    "    \n",
    "    # Save DICOM as NIfTI\n",
    "    subject_path = os.path.join(output_directory, 'subject.nii.gz')\n",
    "    sitk.WriteImage(image, subject_path, True)\n",
    "    subject_nifti = ants.image_read(subject_path)\n",
    "    \n",
    "\n",
    "\n",
    "    prob_brain_mask = brain_extraction(subject_nifti, modality=\"t2\")\n",
    "    brain_mask = ants.get_mask(prob_brain_mask, low_thresh=0.5)\n",
    "    extracted_brain = ants.mask_image(subject_nifti, brain_mask)\n",
    "    output_path = os.path.join(output_directory, \"extracted_brain.nii.gz\")\n",
    "    ants.image_write(extracted_brain, output_path)\n",
    "\n",
    "    mytx = ants.registration(fixed=template_extracted_brain, \n",
    "                         moving=extracted_brain, \n",
    "                         type_of_transform = 'SyN' )\n",
    "    \n",
    "    warped_image = ants.apply_transforms(fixed=template_extracted_brain,\n",
    "                                            moving=extracted_brain,\n",
    "                                            transformlist=mytx['fwdtransforms'])\n",
    "    ants.image_write(warped_image, os.path.join(output_directory, \"warped_brain.nii.gz\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001FDCF82C0D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001FDCF5110D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "dicom_root = 'PPMI'\n",
    "nifti_root = 'NIfTI'\n",
    "\n",
    "for patno in os.listdir(dicom_root):\n",
    "    try:\n",
    "        patno_int = int(patno)\n",
    "        matching_rows = df[df[\"PATNO\"] == patno_int]\n",
    "        if not matching_rows.empty:\n",
    "            patno_path = os.path.join(dicom_root, patno)\n",
    "            if os.path.isdir(patno_path):\n",
    "                for dtype in os.listdir(patno_path):\n",
    "                    type_path = os.path.join(patno_path, dtype)\n",
    "                    if os.path.isdir(type_path):\n",
    "                        for date in os.listdir(type_path):\n",
    "                            date_path = os.path.join(type_path, date)\n",
    "                            if os.path.isdir(date_path):\n",
    "                                for ids in os.listdir(date_path):\n",
    "                                    ids_path = os.path.join(date_path, ids)\n",
    "                                    if os.path.isdir(ids_path):\n",
    "                                        output_path = os.path.join(nifti_root, patno, dtype, date, ids)\n",
    "                                        convert_dicom_to_nifti_and_extract_brain(ids_path, output_path)\n",
    "    except ValueError:\n",
    "        print(f\"PATNO directory: {patno}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import uuid  # Import UUID to generate random strings\n",
    "\n",
    "nifti_root = 'NIfTI'\n",
    "data_root = 'DATA'\n",
    "\n",
    "for patno in os.listdir(nifti_root):\n",
    "    try:\n",
    "        patno_int = int(patno)\n",
    "        matching_rows = df[df[\"PATNO\"] == patno_int]\n",
    "        if not matching_rows.empty:\n",
    "            cohort_value = str(matching_rows[\"COHORT\"].values[0])\n",
    "            cohort_path = os.path.join(data_root, cohort_value)\n",
    "            os.makedirs(cohort_path, exist_ok=True)\n",
    "            patno_path = os.path.join(nifti_root, patno)\n",
    "            if os.path.isdir(patno_path):\n",
    "                dtypes = os.listdir(patno_path)\n",
    "                for dtype in dtypes:\n",
    "                    type_path = os.path.join(patno_path, dtype)\n",
    "                    if os.path.isdir(type_path):\n",
    "                        for date in os.listdir(type_path):\n",
    "                            date_path = os.path.join(type_path, date)\n",
    "                            if os.path.isdir(date_path):\n",
    "                                for ids in os.listdir(date_path):\n",
    "                                    ids_path = os.path.join(date_path, ids)\n",
    "                                    if os.path.isdir(ids_path):\n",
    "                                        file_path = os.path.join(ids_path, 'extracted_brain.nii.gz')\n",
    "                                        if os.path.isfile(file_path):\n",
    "                                            # Generate a unique identifier\n",
    "                                            unique_id = uuid.uuid4().hex\n",
    "                                            new_filename = f\"{unique_id}.nii.gz\"\n",
    "                                            file_target_path = os.path.join(cohort_path, new_filename)\n",
    "                                            shutil.copy(file_path, file_target_path)\n",
    "        else:\n",
    "            print(\"No matching record for PATNO:\", patno_int)\n",
    "    except ValueError:\n",
    "        print(f\"PATNO directory: {patno}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder: 1, Number of files: 534\n",
      "Folder: 2, Number of files: 240\n",
      "Folder: 3, Number of files: 114\n",
      "Folder: 4, Number of files: 69\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "data_root = 'DATA'\n",
    "\n",
    "if os.path.exists(data_root):\n",
    "    for folder in os.listdir(data_root):\n",
    "        folder_path = os.path.join(data_root, folder)\n",
    "        if os.path.isdir(folder_path):\n",
    "            files = [file for file in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, file))]\n",
    "            file_count = len(files)\n",
    "            print(f\"Folder: {folder}, Number of files: {file_count}\")\n",
    "else:\n",
    "    print(\"The directory DATA does not exist.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "source_root = 'DATA'\n",
    "target_root_1 = 'DATA_ORD_1'\n",
    "target_root_2 = 'DATA_ORD_2'\n",
    "\n",
    "\n",
    "os.makedirs(target_root_1, exist_ok=True)\n",
    "os.makedirs(target_root_2, exist_ok=True)\n",
    "\n",
    "if os.path.exists(source_root):\n",
    "    for folder in os.listdir(source_root):\n",
    "        folder_path = os.path.join(source_root, folder)\n",
    "        if os.path.isdir(folder_path):\n",
    "            files = [file for file in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, file))]\n",
    "            files.sort()\n",
    "            first_90_files = files[:90]\n",
    "            remaining_files = files[90:]\n",
    "            def copy_files(file_list, target_root):\n",
    "                target_folder_path = os.path.join(target_root, folder)\n",
    "                os.makedirs(target_folder_path, exist_ok=True)\n",
    "                for file in file_list:\n",
    "                    source_file_path = os.path.join(folder_path, file)\n",
    "                    target_file_path = os.path.join(target_folder_path, file)\n",
    "                    shutil.copy(source_file_path, target_file_path)\n",
    "\n",
    "            copy_files(first_90_files, target_root_1)\n",
    "            copy_files(remaining_files, target_root_2)\n",
    "\n",
    "else:\n",
    "    print(\"does not exist\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder: 1, Number of files: 90\n",
      "Folder: 2, Number of files: 80\n",
      "Folder: 3, Number of files: 38\n",
      "Folder: 4, Number of files: 23\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "data_root = 'DATA_ORD_1'\n",
    "\n",
    "if os.path.exists(data_root):\n",
    "    for folder in os.listdir(data_root):\n",
    "        folder_path = os.path.join(data_root, folder)\n",
    "        if os.path.isdir(folder_path):\n",
    "            files = [file for file in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, file))]\n",
    "            file_count = len(files)\n",
    "            print(f\"Folder: {folder}, Number of files: {file_count}\")\n",
    "else:\n",
    "    print(\"Does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder: 1, Number of files: 88\n",
      "Folder: 2, Number of files: 0\n",
      "Folder: 3, Number of files: 0\n",
      "Folder: 4, Number of files: 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "data_root = 'DATA_ORD_2'\n",
    "\n",
    "if os.path.exists(data_root):\n",
    "    for folder in os.listdir(data_root):\n",
    "        folder_path = os.path.join(data_root, folder)\n",
    "        if os.path.isdir(folder_path):\n",
    "            files = [file for file in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, file))]\n",
    "            file_count = len(files)\n",
    "            print(f\"Folder: {folder}, Number of files: {file_count}\")\n",
    "else:\n",
    "    print(\"Does not exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -r requirements.txt\n",
    "# %pip install -U \"jax[cuda12_pip]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
    "# %pip install --upgrade keras-cv\n",
    "# %pip install --upgrade keras\n",
    "# %pip install pydot\n",
    "# % pip install nybabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import Input, Conv3D, MaxPooling3D, BatchNormalization, GlobalMaxPooling3D, LSTM, Dense, Dropout, Reshape\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "def hybrid():\n",
    "    inputs = Input(shape=(128, 128, 64, 1))\n",
    "\n",
    "    x = Conv3D(64, (3, 3, 3), activation='relu', padding='valid')(inputs)\n",
    "    x = MaxPooling3D((2, 2, 2))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Conv3D(64, (3, 3, 3), activation='relu', padding='valid')(x)\n",
    "    x = MaxPooling3D((2, 2, 2))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Conv3D(128, (3, 3, 3), activation='relu', padding='valid')(x)\n",
    "    x = MaxPooling3D((2, 2, 2))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "\n",
    "    x = Conv3D(256, (3, 3, 3), activation='relu', padding='valid')(x)\n",
    "    x = MaxPooling3D((2, 2, 2))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = GlobalMaxPooling3D()(x)\n",
    "\n",
    "\n",
    "    x = Reshape((1, -1))(x)\n",
    "    x = LSTM(128)(x)\n",
    "\n",
    "\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "\n",
    "\n",
    "    outputs = Dense(4, activation='softmax')(x)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "model = hybrid()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import tensorflow as tf\n",
    "\n",
    "def load_nii_to_array(path, slice_range, output_shape=(128, 128, 64)):\n",
    "    # Load the NIfTI file\n",
    "    nii = nib.load(path)\n",
    "    data = nii.get_fdata()\n",
    "    \n",
    "    # Normalize data\n",
    "    # data = (data - np.min(data)) / (np.max(data) - np.min(data))\n",
    "\n",
    "    # Select the slice range\n",
    "    actual_end_slice = min(slice_range[1], data.shape[2])\n",
    "    data = data[:, :, slice_range[0]:actual_end_slice]\n",
    "    num_slices = data.shape[2]\n",
    "\n",
    "    if num_slices < output_shape[2]:\n",
    "        raise ValueError(f\"Not enough slices available. Needed: {output_shape[2]}, available: {num_slices}.\")\n",
    "\n",
    "    # Correct linspace usage to avoid out-of-bound indices\n",
    "    indices = np.linspace(0, num_slices - 1, output_shape[2], dtype=int)\n",
    "    data = data[:, :, indices]\n",
    "\n",
    "    # Resize slices to 128x128, ensuring each slice has a channel dimension\n",
    "    resized_data = np.zeros((output_shape[0], output_shape[1], output_shape[2], 1))  # Notice the addition of the channel dimension here\n",
    "    for i in range(output_shape[2]):\n",
    "        # Add channel dimension before resizing\n",
    "        slice_with_channel = np.expand_dims(data[:, :, i], axis=-1)\n",
    "        resized_slice = tf.image.resize(slice_with_channel, output_shape[:2], method='bilinear')\n",
    "        resized_data[:, :, i, :] = resized_slice.numpy()  # Storing with channel dimension\n",
    "\n",
    "    return resized_data\n",
    "\n",
    "\n",
    "def process_directory(directory, slice_range):\n",
    "    X = []\n",
    "    y = []\n",
    "    # Iterate over class directories\n",
    "    for class_dir in os.listdir(directory):\n",
    "        class_path = os.path.join(directory, class_dir)\n",
    "        if os.path.isdir(class_path):\n",
    "            class_label = int(class_dir) - 1  # Assuming class directories are 1-indexed\n",
    "            # Iterate over all files in the class directory\n",
    "            for filename in os.listdir(class_path):\n",
    "                if filename.endswith(\".nii.gz\"):\n",
    "                    file_path = os.path.join(class_path, filename)\n",
    "                    slices = load_nii_to_array(file_path, slice_range)\n",
    "                    X.append(slices)\n",
    "                    y.append(class_label)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Load training and validation data\n",
    "data_dir = \"DATA_ORD_1\"\n",
    "slice_range = (58, 122)\n",
    "\n",
    "data, labels = process_directory(data_dir, slice_range)\n",
    "\n",
    "# Shuffle and split data\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "split = int(0.8 * len(data))  # 80% training, 20% validation\n",
    "train_data, train_labels = data[:split], labels[:split]\n",
    "val_data, val_labels = data[split:], labels[split:]\n",
    "\n",
    "batch_size = 2\n",
    "\n",
    "# Convert to TensorFlow datasets\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((train_data, train_labels)).batch(batch_size)\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((val_data, val_labels)).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the augmentation layers\n",
    "data_augmentation_layers = [\n",
    "    tf.keras.layers.RandomRotation(0.1),\n",
    "]\n",
    "\n",
    "# Function to apply data augmentation\n",
    "def data_augmentation(images):\n",
    "    for layer in data_augmentation_layers:\n",
    "        images = layer(images, training=True)  # Ensure layers are in training mode to perform augmentation\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def random_flip_3d(image):\n",
    "    \"\"\"Randomly flip a 3D image along different axes.\"\"\"\n",
    "    image = tf.cond(tf.random.uniform([]) < 0.5, lambda: tf.reverse(image, axis=[1]), lambda: image)  # Flip along height\n",
    "    image = tf.cond(tf.random.uniform([]) < 0.5, lambda: tf.reverse(image, axis=[2]), lambda: image)  # Flip along width\n",
    "    image = tf.cond(tf.random.uniform([]) < 0.5, lambda: tf.reverse(image, axis=[3]), lambda: image)  # Flip along depth\n",
    "    return image\n",
    "\n",
    "# Apply data augmentation to the training dataset\n",
    "# train_ds = train_ds.map(\n",
    "#     lambda img, label: (random_flip_3d(img), label),\n",
    "#     num_parallel_calls=tf.data.AUTOTUNE\n",
    "# )\n",
    "\n",
    "# Prefetching to prepare data while training\n",
    "train_ds = train_ds.prefetch(tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 35\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"save_at_{epoch}.keras\"),\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=3e-4),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=val_ds,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"DATA_ORD_2\"\n",
    "slice_range = (58, 122)\n",
    "\n",
    "data, labels = process_directory(data_dir, slice_range)\n",
    "\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "test_data, test_labels = data, labels\n",
    "batch_size = 2\n",
    "\n",
    "\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((test_data, train_labels)).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_ds)\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_ds)\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "test_labels = np.concatenate([y for x, y in test_ds], axis=0)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "conf_matrix = confusion_matrix(test_labels, predicted_labels)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"\\nClassification Report:\\n\", classification_report(test_labels, predicted_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
